
@article{delicado_another_2001,
	title = {Another {Look} at {Principal} {Curves} and {Surfaces}},
	volume = {77},
	issn = {0047-259X},
	doi = {10.1006/jmva.2000.1917},
	abstract = {Principal curves have been defined as smooth curves passing through the “middle” of a multidimensional data set. They are nonlinear generalizations of the first principal component, a characterization of which is the basis of the definition of principal curves. We establish a new characterization of the first principal component and base our new definition of a principal curve on this property. We introduce the notion of principal oriented points and we prove the existence of principal curves passing through these points. We extend the definition of principal curves to multivariate data sets and propose an algorithm to find them. The new notions lead us to generalize the definition of total variance. Successive principal curves are recursively defined from this generalization. The new methods are illustrated on simulated and real data sets.},
	language = {en},
	number = {1},
	urldate = {2022-11-29},
	journal = {Journal of Multivariate Analysis},
	author = {Delicado, Pedro},
	month = apr,
	year = {2001},
	keywords = {fixed points, generalized total variance, nonlinear multivariate analysis, principal components, smoothing techniques},
	pages = {84--116},
}

@article{smola_regularized_nodate,
	title = {Regularized {Principal} {Manifolds}},
	abstract = {Many settings of unsupervised learning can be viewed as quantization problems - the minimization of the expected quantization error subject to some restrictions. This allows the use of tools such as regularization from the theory of (supervised) risk minimization for unsupervised learning. This setting turns out to be closely related to principal curves, the generative topographic map, and robust coding.},
	language = {en},
	author = {Smola, Alexander J and Mika, Sebastian and Scholkopf, Bernhard and Williamson, Robert C},
	pages = {31},
}

@article{ozertem_locally_nodate,
	title = {Locally {Deﬁned} {Principal} {Curves} and {Surfaces}},
	abstract = {Principal curves are deﬁned as self-consistent smooth curves passing through the middle of the data, and they have been used in many applications of machine learning as a generalization, dimensionality reduction and a feature extraction tool. We redeﬁne principal curves and surfaces in terms of the gradient and the Hessian of the probability density estimate. This provides a geometric understanding of the principal curves and surfaces, as well as a unifying view for clustering, principal curve ﬁtting and manifold learning by regarding those as principal manifolds of different intrinsic dimensionalities. The theory does not impose any particular density estimation method can be used with any density estimator that gives continuous ﬁrst and second derivatives. Therefore, we ﬁrst present our principal curve/surface deﬁnition without assuming any particular density estimation method. Afterwards, we develop practical algorithms for the commonly used kernel density estimation (KDE) and Gaussian mixture models (GMM). Results of these algorithms are presented in notional data sets as well as real applications with comparisons to other approaches in the principal curve literature. All in all, we present a novel theoretical understanding of principal curves and surfaces, practical algorithms as general purpose machine learning tools, and applications of these algorithms to several practical problems.},
	language = {en},
	author = {Ozertem, Umut and Erdogmus, Deniz},
	pages = {38},
}

@article{tibshirani_principal_1992,
	title = {Principal curves revisited},
	volume = {2},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/BF01889678},
	doi = {10.1007/BF01889678},
	abstract = {A principal curve (Hastie and Stuetzle, 1989) is a smooth curve passing through the 'middle' of a distribution or data cloud, and is a generalization of linear principal components. We give an alternative definition of a principal curve, based on a mixture model. Estimation is carried out through an EM algorithm. Some comparisons are made to the Hastie-Stuetzle definition.},
	language = {en},
	number = {4},
	urldate = {2022-11-29},
	journal = {Statistics and Computing},
	author = {Tibshirani, Robert},
	month = dec,
	year = {1992},
	pages = {183--190},
}

@article{hastie_principal_1989,
	title = {Principal {Curves}},
	volume = {84},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478797},
	doi = {10.1080/01621459.1989.10478797},
	language = {en},
	number = {406},
	urldate = {2022-11-29},
	journal = {Journal of the American Statistical Association},
	author = {Hastie, Trevor and Stuetzle, Werner},
	month = jun,
	year = {1989},
	pages = {502--516},
}

@techreport{hastie_principal_1985,
	address = {Seattle, Washington},
	type = {Technical {Report}},
	title = {Principal {Curves} and {Surfaces}},
	url = {https://stat.uw.edu/sites/default/files/files/reports/1985/tr056.pdf},
	abstract = {Principal curves are smooth one dimensional curves that pass through the middle of a p dimensional data set. They provide a non-linear summary of the data. The curves are non-parametric and their shape is suggested by the data. Similarly, principal surfaces are two dimensional surfaces that pass through the middle of the data. The curves and surfaces are found using an iterative procedure which starts with some prior summary such as the usual principal component line or plane. Each successive iteration is a smooth or local average of the p dimensional points, where local is based on the projections of the points onto the curve or surface of the previous iteration.

In this paper we develop some theory for principal curves and surfaces, and present an algorithm for estimating them. The main theorem states that principal curves are critical values of the expected squared distance between the points and the curve. We compare the principal curve and surface procedures to other generalizations of principal components in the literature; the usual generalizations transform the space, whereas we transform the model. There are also strong ties with multidimensional scaling.},
	number = {56},
	institution = {University of Washington Department of Statistics},
	author = {Hastie, Trevor and Stuetzle, Werner},
	month = jan,
	year = {1985},
}

@techreport{hastie_principal_1984,
	type = {Technical {Report}},
	title = {Principal {Curves} and {Surfaces}},
	abstract = {Principal curves are smooth one dimensional curves that pass through the middle of a p dimensional data set. They minimize the distance from the points, and provide a non-linear summary of the data. The curves are non-parametric and their shape is suggested by the data. Similarly, principal surfaces are two dimensional surfaces that pass through the middle of the data. The curves and surfaces are found using an iterative procedure which starts with a linear summary such as the usual principal component line or plane. Each successive iteration is a smooth or local average of the p dimensional points, where local is based on the projections of the points onto the curve or surface of the previous iteration.

A number of linear techniques, such as factor analysis and errors in variables regression, end up using the principal components as their estimates (after a suitable scaling of the co-ordinates). Principal curves and surfaces can be viewed as the estimates of non-linear generalizations of these procedures. We present some real data examples that illustrate these applications.

Principal Curves (or surfaces) have a theoretical definition for distributions: they are the self consistent curves. A curve is self consistent if each point on the curve is the conditional mean of the points that project there. The main theorem proves that principal curves are critical values of the expected squared distance between the points and the curve. Linear principal components have this property as well; in fact, we prove taht if a principal curve is straight, then it is a principal component. These results generalize the usual duality between conditional expectation and distance minimization. We also examine two sources of bias in the procedures, which have the satisfactory property of partially cancelling each other.

We compare the principal curve and surface procedures to other generalizations of principal components in the literature; the usual generalizations transform the space, whereas we transform the model. There are also strong ties with multidimensional scaling.},
	number = {11},
	institution = {Stanford University Lab for Computational Statistics},
	author = {Hastie, Trevor},
	month = nov,
	year = {1984},
}

@article{noauthor_notitle_nodate,
}

@misc{university_principal_nodate,
	title = {Principal {Curves} and {Surfaces}},
	url = {https://purl.stanford.edu/qx227zb1711},
	language = {en},
	urldate = {2022-11-29},
	author = {University, © Stanford and {Stanford} and California 94305},
}

@article{meng_principal_2021,
	title = {Principal manifold estimation via model complexity selection},
	volume = {83},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12416},
	doi = {10.1111/rssb.12416},
	abstract = {We propose a framework of principal manifolds to model high-dimensional data. This framework is based on Sobolev spaces and designed to model data of any intrinsic dimension. It includes principal component analysis and principal curve algorithm as special cases. We propose a novel method for model complexity selection to avoid overfitting, eliminate the effects of outliers and improve the computation speed. Additionally, we propose a method for identifying the interiors of circle-like curves and cylinder/ball-like surfaces. The proposed approach is compared to existing methods by simulations and applied to estimate tumour surfaces and interiors in a lung cancer study.},
	language = {en},
	number = {2},
	urldate = {2022-11-29},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Meng, Kun and Eloyan, Ani},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12416},
	keywords = {lung cancer, splines, total squared curvature, tumour interior},
	pages = {369--394},
}

@article{meng_principal_2021-1,
	title = {Principal manifold estimation via model complexity selection},
	volume = {83},
	issn = {1369-7412, 1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/rssb.12416},
	doi = {10.1111/rssb.12416},
	abstract = {We propose a framework of principal manifolds to model high-dimensional data. This framework is based on Sobolev spaces and designed to model data of any intrinsic dimension. It includes principal component analysis and principal curve algorithm as special cases. We propose a novel method for model complexity selection to avoid overfitting, eliminate the effects of outliers, and improve the computation speed. Additionally, we propose a method for identifying the interiors of circle-like curves and cylinder/ball-like surfaces. The proposed approach is compared to existing methods by simulations and applied to estimate tumor surfaces and interiors in a lung cancer study.},
	language = {en},
	number = {2},
	urldate = {2022-11-29},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Meng, Kun and Eloyan, Ani},
	month = apr,
	year = {2021},
	pages = {369--394},
}

@article{meng_principal_nodate,
	title = {Principal manifold estimation via model complexity selection},
	abstract = {We propose a framework of principal manifolds to},
	author = {Meng, Kun and Eloyan, Ani},
}
